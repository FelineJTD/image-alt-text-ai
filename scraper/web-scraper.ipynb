{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make a web scraper!\n",
    "\n",
    "## Goals: \n",
    "- Get data from a few websites, perhaps 100 images first, with the surrounding context (closest tag to them, class names, image names, parent tag, closest p for textual context and how far it is perhaps?)\n",
    "\n",
    "## How to:\n",
    "- Collect website names, just go for random 50 websites\n",
    "- Scrape them, analyze (wholly)\n",
    "- Decide which parts to take\n",
    "- Compile to json\n",
    "\n",
    "## To do:\n",
    "- [v] Compile a list of websites with a more diverse role  \n",
    "- [] Scrape parent tag  \n",
    "- [] Scrape textual content  \n",
    "- [v] Compile to JSON or smth else  \n",
    "- [] Download the images and store in a folder  \n",
    "- [] Git ignore outputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /home/felinejtd/.local/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/felinejtd/.local/lib/python3.10/site-packages (4.12.3)\n",
      "Requirement already satisfied: matplotlib in /home/felinejtd/.local/lib/python3.10/site-packages (3.9.0)\n",
      "Requirement already satisfied: numpy in /home/felinejtd/.local/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/felinejtd/.local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/felinejtd/.local/lib/python3.10/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/felinejtd/.local/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/felinejtd/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/felinejtd/.local/lib/python3.10/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/felinejtd/.local/lib/python3.10/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/felinejtd/.local/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/felinejtd/.local/lib/python3.10/site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/felinejtd/.local/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/felinejtd/.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/felinejtd/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/felinejtd/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 matplotlib numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://dukaafricana.com', 'http://ludacreative.com.au', 'http://theitalianschool.co.uk', 'http://mcz.edu.pl', 'http://donutworrybehappy.eu']\n"
     ]
    }
   ],
   "source": [
    "# Open ./website_url_data/builtwith-top1m-20240621.csv and read all the URLs\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./website_url_data/builtwith-top1m-20240621-random.csv')\n",
    "websites = df['url'].tolist()\n",
    "\n",
    "# Append http:// to each URL\n",
    "websites = ['http://' + url for url in websites]\n",
    "\n",
    "# Print the first 5 URLs\n",
    "print(websites[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to label image manually\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def label_image(image_url, image_alt, file_name):\n",
    "    # Download the image\n",
    "    image_res = requests.get(image_url)\n",
    "    # Save the image to a file\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(image_res.content)\n",
    "    # Display the image\n",
    "    image = Image.open(BytesIO(image_res.content))\n",
    "    plt.imshow(image)\n",
    "    plt.title(f'Current alt: {image_alt}')\n",
    "    plt.axis(\"off\")\n",
    "    # Set bg color to gray\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('gray')\n",
    "    plt.show()\n",
    "    # Manually label the data\n",
    "    # 1. Role\n",
    "    #   1: Informative (images that graphically represent concepts and information, typically pictures, photos, and illustrations.)\n",
    "    #   2: Decorative (the only purpose of an image is to add visual decoration to the page)\n",
    "    #   3: Functional (image used as a link or as a button, should describe the functionality of the link or button rather than the visual image)\n",
    "    #   4: Text (readable text)\n",
    "    #   5: Complex (diagrams or graphs)\n",
    "    role = input(\"Role of image (1-5): \")\n",
    "    # A shortcut to skip the image, input '0' for role\n",
    "    if (role == '0'):\n",
    "        return None\n",
    "    # A shortcut to cut the loop, input 'q' for role\n",
    "    if (role == 'q'):\n",
    "        return 'q'\n",
    "    \n",
    "    # 2. New alt text, if previous alt text is not descriptive enough\n",
    "    new_alt = input(\"New alt text: \")\n",
    "\n",
    "    # 3. If the image has a readable text, input the text\n",
    "    # text = input(\"Text in image (if any): \")\n",
    "\n",
    "    # 4. If the image has a known entity, input the entity\n",
    "    # entity = input(\"Entity in image (if any, comma separated): \")\n",
    "\n",
    "    # Parse the data\n",
    "    if (role == '1'):\n",
    "        role = 'informative'\n",
    "    elif (role == '2'):\n",
    "        role = 'decorative'\n",
    "    elif (role == '3'):\n",
    "        role = 'functional'\n",
    "    elif (role == '4'):\n",
    "        role = 'text'\n",
    "    elif (role == '5'):\n",
    "        role = 'complex'\n",
    "    else:\n",
    "        role = 'unknown'\n",
    "\n",
    "    if (new_alt == ''):\n",
    "        new_alt = image_alt\n",
    "\n",
    "    # entity = entity.split(',')\n",
    "\n",
    "    data = {\n",
    "        'role': role,\n",
    "        'alt': new_alt,\n",
    "    }\n",
    "\n",
    "    return (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 58 of 58, website 1 of 1000000\n",
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "import webbrowser\n",
    "import os\n",
    "\n",
    "i_website = 1\n",
    "# Load the progress\n",
    "# try:\n",
    "#     with open(\"progress.txt\", \"r\") as f:\n",
    "#         i_website = int(f.readline())\n",
    "# except FileNotFoundError:\n",
    "#     pass\n",
    "\n",
    "for i in range(i_website, len(websites)):\n",
    "    website_info = {}\n",
    "    try:\n",
    "        # Open the URL in a new tab for reference while labelling\n",
    "        webbrowser.open_new_tab(websites[i])\n",
    "        # Skip website?\n",
    "        skip = input(\"Skip website? (input anything to skip): \")\n",
    "        if skip == 'q':\n",
    "            # Save the progress\n",
    "            with open(\"progress.txt\", \"w\") as f:\n",
    "                f.write(str(i_website))\n",
    "            break\n",
    "        if skip:\n",
    "            i_website += 1\n",
    "            continue\n",
    "\n",
    "        # Initialize the list to store the image data\n",
    "        images_info = []\n",
    "        # Get the HTML content of the page\n",
    "        page = requests.get(websites[i])\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "        # Get all text in the page\n",
    "        text = soup.get_text()\n",
    "        website_info['text'] = text\n",
    "\n",
    "        # Create a folder in ./images/ with the name of the website\n",
    "        os.makedirs(f\"./images/{websites[i].split('//')[1].replace('/', '-')}\", exist_ok=True)\n",
    "\n",
    "        # Find all 'img' tags\n",
    "        images = soup.find_all(\"img\")\n",
    "        # Remove duplicates\n",
    "        images = set(images)\n",
    "\n",
    "\n",
    "        i_image = 1\n",
    "        for image in images:\n",
    "            try:\n",
    "                # GET RELEVANT DATA OF THE IMAGE\n",
    "                # Clear the output before displaying the next image to avoid an overly big notebook size\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                print(f\"Image {i_image} of {len(images)}, website {i_website} of {len(websites)}\")\n",
    "\n",
    "                # The 'src' attribute of the image\n",
    "                image_url = image[\"src\"]\n",
    "                print(\"Image URL: \", image_url)\n",
    "                # Relative path handling\n",
    "                if not image_url.startswith(('http://', 'https://')):\n",
    "                    image_url = urljoin(websites[i], image_url)\n",
    "\n",
    "                # The 'alt' attribute of the image\n",
    "                image_alt = image.get(\"alt\", \"No alt attribute\")  # Use a default value if 'alt' is missing\n",
    "\n",
    "                # The image file name\n",
    "                file_name = f\"./images/{websites[i].split('//')[1].replace('/', '-')}/image_{i_image}.jpg\"\n",
    "\n",
    "                # LABELLING (ROLE AND ALT TEXT)\n",
    "                # Label the image manually\n",
    "                data = label_image(image_url, image_alt, file_name)\n",
    "                if data == 'q':\n",
    "                    # Save the progress\n",
    "                    with open(\"progress.txt\", \"w\") as f:\n",
    "                        f.write(str(i_website))\n",
    "                        f.write(\"\\n\")\n",
    "                        f.write(str(i_image))\n",
    "                    # Close the browser\n",
    "                    # webbrowser.close()\n",
    "                    break\n",
    "                if data is None:\n",
    "                    continue\n",
    "\n",
    "                # OTHER IMAGE ATTRIBUTES\n",
    "                image_attrs = image.attrs\n",
    "\n",
    "                # Find out if image has <a> parent or <button> parent, potentially indicating a functional image\n",
    "                a_button_parent_found = False\n",
    "                # Loop up to 3 levels up the hierarchy\n",
    "                current_tag = image\n",
    "                a_button_parent = None\n",
    "                for _ in range(3):\n",
    "                    # Try to find a parent <a> tag or <button> tag\n",
    "                    a_button_parent = current_tag.find_parent([\"a\", \"button\"])\n",
    "                    if a_button_parent:\n",
    "                        # If an <a> or <button> parent is found, set the flag to True and break the loop\n",
    "                        a_button_parent_found = True\n",
    "                        break\n",
    "                    else:\n",
    "                        # If not found, move up to the next parent\n",
    "                        current_tag = current_tag.parent\n",
    "                        # If the current tag is None (top of the tree), break the loop\n",
    "                        if current_tag is None:\n",
    "                            break\n",
    "                print(\"Parent <a> or <button> found: \", a_button_parent_found)\n",
    "                print(\"Parent tag: \", a_button_parent)\n",
    "\n",
    "                # NEAREST TEXT FROM IMAGE FOR TEXT CONTEXT\n",
    "                previous_text = image.find_previous([\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "                next_text = image.find_next([\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "\n",
    "                # Write the data (image and labels) to a file\n",
    "                images_info.append({\n",
    "                    \"src\": image_url,\n",
    "                    \"file_name\": file_name,\n",
    "                    \"role\": data['role'],\n",
    "                    \"alt\": data['alt'],\n",
    "                    \"attrs\": image_attrs,\n",
    "                    \"a_button_parent\": str(a_button_parent),\n",
    "                    \"previous_text\": previous_text.text if previous_text else \"\",\n",
    "                    \"next_text\": next_text.text if next_text else \"\",\n",
    "                })\n",
    "                print(\"Data saved\")\n",
    "                \n",
    "            except KeyError:\n",
    "                # Put to log file\n",
    "                with open(\"error.log\", \"a\") as f:\n",
    "                    f.write(f\"Error at image {i_image} of website {i_website}: Key error\")\n",
    "                    if image_url:\n",
    "                        f.write(f\"Image URL: {image_url}\\n\")\n",
    "            except requests.exceptions.MissingSchema:\n",
    "                # Put to log file\n",
    "                with open(\"error.log\", \"a\") as f:\n",
    "                    f.write(f\"Error at image {i_image} of website {i_website}: Missing schema\\n\")\n",
    "                    if image_url:\n",
    "                        f.write(f\"Image URL: {image_url}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                # Put to log file\n",
    "                with open(\"error.log\", \"a\") as f:\n",
    "                    f.write(f\"Error at image {i_image} of website {i_website}: {e}\\n\")\n",
    "                    if image_url:\n",
    "                        f.write(f\"Image URL: {image_url}\\n\")\n",
    "\n",
    "            i_image += 1\n",
    "\n",
    "        website_info['images'] = images_info\n",
    "        # Step 4: Write the list to a file in JSON format\n",
    "        with open(f\"./output/{websites[i].split('//')[1].replace('/', '-')}.json\", \"w\") as f:\n",
    "            json.dump(website_info, f, indent=4)\n",
    "\n",
    "        i_website += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        i_website += 1\n",
    "        # Put to log file\n",
    "        with open(\"error.log\", \"a\") as f:\n",
    "            f.write(f\"Error at website {i_website}: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "\u001b[33mDEPRECATION: The HTML index page being used (https://download.pytorch.org/whl/torch_stable.html) is not a proper HTML 5 document. This is in violation of PEP 503 which requires these pages to be well-formed HTML 5 documents. Please reach out to the owners of this index page, and ask them to update this index page to a valid HTML 5 document. pip 22.2 will enforce this behaviour change. Discussion can be found at https://github.com/pypa/pip/issues/10825\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.7.1+cpu (from versions: 1.11.0, 1.11.0+cpu, 1.11.0+cu102, 1.11.0+cu113, 1.11.0+cu115, 1.11.0+rocm4.3.1, 1.11.0+rocm4.5.2, 1.12.0, 1.12.0+cpu, 1.12.0+cu102, 1.12.0+cu113, 1.12.0+cu116, 1.12.0+rocm5.0, 1.12.0+rocm5.1.1, 1.12.1, 1.12.1+cpu, 1.12.1+cu102, 1.12.1+cu113, 1.12.1+cu116, 1.12.1+rocm5.0, 1.12.1+rocm5.1.1, 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.0+cu117.with.pypi.cudnn, 1.13.0+rocm5.1.1, 1.13.0+rocm5.2, 1.13.1, 1.13.1+cpu, 1.13.1+cu116, 1.13.1+cu117, 1.13.1+cu117.with.pypi.cudnn, 1.13.1+rocm5.1.1, 1.13.1+rocm5.2, 2.0.0, 2.0.0+cpu, 2.0.0+cpu.cxx11.abi, 2.0.0+cu117, 2.0.0+cu117.with.pypi.cudnn, 2.0.0+cu118, 2.0.0+rocm5.3, 2.0.0+rocm5.4.2, 2.0.1, 2.0.1+cpu, 2.0.1+cpu.cxx11.abi, 2.0.1+cu117, 2.0.1+cu117.with.pypi.cudnn, 2.0.1+cu118, 2.0.1+rocm5.3, 2.0.1+rocm5.4.2, 2.1.0, 2.1.0+cpu, 2.1.0+cpu.cxx11.abi, 2.1.0+cu118, 2.1.0+cu121, 2.1.0+cu121.with.pypi.cudnn, 2.1.0+rocm5.5, 2.1.0+rocm5.6, 2.1.1, 2.1.1+cpu, 2.1.1+cpu.cxx11.abi, 2.1.1+cu118, 2.1.1+cu121, 2.1.1+cu121.with.pypi.cudnn, 2.1.1+rocm5.5, 2.1.1+rocm5.6, 2.1.2, 2.1.2+cpu, 2.1.2+cpu.cxx11.abi, 2.1.2+cu118, 2.1.2+cu121, 2.1.2+cu121.with.pypi.cudnn, 2.1.2+rocm5.5, 2.1.2+rocm5.6, 2.2.0, 2.2.0+cpu, 2.2.0+cpu.cxx11.abi, 2.2.0+cu118, 2.2.0+cu121, 2.2.0+rocm5.6, 2.2.0+rocm5.7, 2.2.1, 2.2.1+cpu, 2.2.1+cpu.cxx11.abi, 2.2.1+cu118, 2.2.1+cu121, 2.2.1+rocm5.6, 2.2.1+rocm5.7, 2.2.2, 2.2.2+cpu, 2.2.2+cpu.cxx11.abi, 2.2.2+cu118, 2.2.2+cu121, 2.2.2+rocm5.6, 2.2.2+rocm5.7, 2.3.0, 2.3.0+cpu, 2.3.0+cpu.cxx11.abi, 2.3.0+cu118, 2.3.0+cu121, 2.3.0+rocm5.7, 2.3.0+rocm6.0, 2.3.1, 2.3.1+cpu, 2.3.1+cpu.cxx11.abi, 2.3.1+cu118, 2.3.1+cu121, 2.3.1+rocm5.7, 2.3.1+rocm6.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.7.1+cpu\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Torch version: 2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# installing some dependencies, CLIP was released in PyTorch\n",
    "import subprocess\n",
    "\n",
    "# CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
    "# print(\"CUDA version:\", CUDA_version)\n",
    "\n",
    "# if CUDA_version == \"10.0\":\n",
    "#     torch_version_suffix = \"+cu100\"\n",
    "# elif CUDA_version == \"10.1\":\n",
    "#     torch_version_suffix = \"+cu101\"\n",
    "# elif CUDA_version == \"10.2\":\n",
    "#     torch_version_suffix = \"\"\n",
    "# else:\n",
    "#     torch_version_suffix = \"+cu110\"\n",
    "\n",
    "%pip install torch==1.7.1+cpu torchvision==0.8.2+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gdown in /home/felinejtd/.local/lib/python3.10/site-packages (5.2.0)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex in /home/felinejtd/.local/lib/python3.10/site-packages (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /home/felinejtd/.local/lib/python3.10/site-packages (from gdown) (4.66.4)\n",
      "Requirement already satisfied: filelock in /home/felinejtd/.local/lib/python3.10/site-packages (from gdown) (3.14.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/felinejtd/.local/lib/python3.10/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: requests[socks] in /home/felinejtd/.local/lib/python3.10/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /home/felinejtd/.local/lib/python3.10/site-packages (from ftfy) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/felinejtd/.local/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/felinejtd/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (2020.6.20)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/felinejtd/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Installing collected packages: ftfy\n",
      "Successfully installed ftfy-6.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gdown ftfy regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'CLIP' already exists and is not an empty directory.\n",
      "CLIP dir is: /home/felinejtd/projects/itb/ta/image-alt-text-ai/CLIP\n"
     ]
    }
   ],
   "source": [
    "# clone the CLIP repository\n",
    "!git clone https://github.com/openai/CLIP.git\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "clip_dir = Path(\".\").absolute() / \"CLIP\"\n",
    "sys.path.append(str(clip_dir))\n",
    "print(f\"CLIP dir is: {clip_dir}\")\n",
    "\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dir: /home/felinejtd/.cache/clip\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, transform = clip.load(\"ViT-B/32\", device=device)\n",
    "print(f\"Model dir: {os.path.expanduser('~/.cache/clip')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
