{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers_stream_generator in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (0.0.5)\n",
      "Requirement already satisfied: transformers>=4.26.1 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from transformers_stream_generator) (4.45.1)\n",
      "Requirement already satisfied: filelock in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from transformers>=4.26.1->transformers_stream_generator) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from transformers>=4.26.1->transformers_stream_generator) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from transformers>=4.26.1->transformers_stream_generator) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from transformers>=4.26.1->transformers_stream_generator) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from transformers>=4.26.1->transformers_stream_generator) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from transformers>=4.26.1->transformers_stream_generator) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from transformers>=4.26.1->transformers_stream_generator) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from transformers>=4.26.1->transformers_stream_generator) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from transformers>=4.26.1->transformers_stream_generator) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from transformers>=4.26.1->transformers_stream_generator) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.26.1->transformers_stream_generator) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.26.1->transformers_stream_generator) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: accelerate in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from accelerate) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from accelerate) (2.2.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from accelerate) (0.25.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.68)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tiktoken in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: einops in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (0.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers_stream_generator\n",
    "%pip install accelerate\n",
    "%pip install tiktoken\n",
    "%pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_40119/3759834011.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 8/8 [1:19:58<00:00, 599.86s/it]\n",
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:11<00:00,  1.49s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You can't move a model that has some modules offloaded to cpu or disk.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_HUMAN_:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuman:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ASSISTANT_:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m---> 57\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mQWenAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m agent\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDraw me a picture of rivers and lakes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m, in \u001b[0;36mQWenAgent.__init__\u001b[0;34m(self, chat_prompt_template, run_prompt_template, additional_tools)\u001b[0m\n\u001b[1;32m     31\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen-7B-Chat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m GenerationConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# 可指定不同的生成长度、top_p等相关超参\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# greedy\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/accelerate/big_modeling.py:456\u001b[0m, in \u001b[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You can't move a model that has some modules offloaded to cpu or disk."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Agent\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "\n",
    "class QWenAgent(Agent):\n",
    "    \"\"\"\n",
    "    Agent that uses QWen model and tokenizer to generate code.\n",
    "\n",
    "    Args:\n",
    "        chat_prompt_template (`str`, *optional*):\n",
    "            Pass along your own prompt if you want to override the default template for the `chat` method. Can be the\n",
    "            actual prompt template or a repo ID (on the Hugging Face Hub). The prompt should be in a file named\n",
    "            `chat_prompt_template.txt` in this repo in this case.\n",
    "        run_prompt_template (`str`, *optional*):\n",
    "            Pass along your own prompt if you want to override the default template for the `run` method. Can be the\n",
    "            actual prompt template or a repo ID (on the Hugging Face Hub). The prompt should be in a file named\n",
    "            `run_prompt_template.txt` in this repo in this case.\n",
    "        additional_tools ([`Tool`], list of tools or dictionary with tool values, *optional*):\n",
    "            Any additional tools to include on top of the default ones. If you pass along a tool with the same name as\n",
    "            one of the default tools, that default tool will be overridden.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```py\n",
    "    agent = QWenAgent()\n",
    "    agent.run(\"Draw me a picture of rivers and lakes.\")\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, chat_prompt_template=None, run_prompt_template=None, additional_tools=None):\n",
    "        checkpoint = \"Qwen/Qwen-7B-Chat\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", trust_remote_code=True).cuda().eval()\n",
    "        self.model.generation_config = GenerationConfig.from_pretrained(checkpoint, trust_remote_code=True) # 可指定不同的生成长度、top_p等相关超参\n",
    "        self.model.generation_config.do_sample = False  # greedy\n",
    "        \n",
    "        super().__init__(\n",
    "            chat_prompt_template=chat_prompt_template,\n",
    "            run_prompt_template=run_prompt_template,\n",
    "            additional_tools=additional_tools,\n",
    "        )\n",
    "\n",
    "    def generate_one(self, prompt, stop):\n",
    "        # \"Human:\" 和 \"Assistant:\" 曾为通义千问的特殊保留字，需要替换为 \"_HUMAN_:\" 和 \"_ASSISTANT_:\"。这一问题将在未来版本修复。\n",
    "        prompt = prompt.replace(\"Human:\", \"_HUMAN_:\").replace(\"Assistant:\", \"_ASSISTANT_:\")\n",
    "        stop = [item.replace(\"Human:\", \"_HUMAN_:\").replace(\"Assistant:\", \"_ASSISTANT_:\") for item in stop]\n",
    "\n",
    "        result, _ = self.model.chat(self.tokenizer, prompt, history=None)\n",
    "        for stop_seq in stop:\n",
    "            if result.endswith(stop_seq):\n",
    "                result = result[: -len(stop_seq)]\n",
    "\n",
    "        result = result.replace(\"_HUMAN_:\", \"Human:\").replace(\"_ASSISTANT_:\", \"Assistant:\")\n",
    "        return result\n",
    "\n",
    "\n",
    "agent = QWenAgent()\n",
    "agent.run(\"Draw me a picture of rivers and lakes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio_client\n",
      "  Downloading gradio_client-1.4.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: fsspec in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from gradio_client) (2024.9.0)\n",
      "Collecting httpx>=0.24.1 (from gradio_client)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from gradio_client) (0.25.1)\n",
      "Requirement already satisfied: packaging in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from gradio_client) (24.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from gradio_client) (4.12.2)\n",
      "Collecting websockets<13.0,>=10.0 (from gradio_client)\n",
      "  Downloading websockets-12.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting anyio (from httpx>=0.24.1->gradio_client)\n",
      "  Downloading anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from httpx>=0.24.1->gradio_client) (2024.8.30)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio_client)\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from httpx>=0.24.1->gradio_client) (3.10)\n",
      "Requirement already satisfied: sniffio in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from httpx>=0.24.1->gradio_client) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio_client) (0.14.0)\n",
      "Requirement already satisfied: filelock in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->gradio_client) (3.16.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->gradio_client) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->gradio_client) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->gradio_client) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->gradio_client) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/felinejtd/anaconda3/envs/thesis/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->gradio_client) (2.2.3)\n",
      "Downloading gradio_client-1.4.0-py3-none-any.whl (319 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "Downloading websockets-12.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Downloading anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "Installing collected packages: websockets, httpcore, anyio, httpx, gradio_client\n",
      "Successfully installed anyio-4.6.2.post1 gradio_client-1.4.0 httpcore-1.0.6 httpx-0.27.2 websockets-12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gradio_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://abidlabs-whisper-large-v2.hf.space ✔\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n..'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "\n",
    "client = Client(\"abidlabs/whisper-large-v2\")  # connecting to a Hugging Face Space\n",
    "client.predict(\"test.mp4\", api_name=\"/predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://qwen-qwen2-vl.hf.space ✔\n",
      "[['Hello!!', None]]\n"
     ]
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "\n",
    "client = Client(\"Qwen/Qwen2-VL\")\n",
    "result = client.predict(\n",
    "\t\thistory=[],\n",
    "\t\ttext=\"Hello!!\",\n",
    "\t\tapi_name=\"/add_text\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'file': '/tmp/gradio/c51f8dda6ee8a458f4a5a8e53e19c98393f66f84c8e4c199a6b859782db3a97a/sample_file.pdf', 'alt_text': None}, None]]\n"
     ]
    }
   ],
   "source": [
    "from gradio_client import Client, handle_file\n",
    "\n",
    "result = client.predict(\n",
    "\t\thistory=[],\n",
    "\t\tfile=handle_file('https://github.com/gradio-app/gradio/raw/main/test/test_files/sample_file.pdf'),\n",
    "\t\tapi_name=\"/add_file\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Tell me what you see in that image.', None]]\n"
     ]
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "\n",
    "result = client.predict(\n",
    "\t\thistory=[],\n",
    "\t\ttext=\"Tell me what you see in that image.\",\n",
    "\t\tapi_name=\"/add_text\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://qwen-qwen2-vl.hf.space ✔\n"
     ]
    },
    {
     "ename": "AppError",
     "evalue": "The upstream Gradio app has raised an exception but has not enabled verbose error reporting. To enable, set show_error=True in launch().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAppError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# client.predict(\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#   history=[[{\"file\":handle_file('https://qwen-qwen2-vl.hf.space/file=/tmp/gradio/1488547f1f4ebb91e5ec06a9bad862268984b714e0286ffed1ed02deb9d128f5/Upper Kolors 1.png'),\"alt_text\":None},None],[\"Tell me what you see in that image\",\"The image shows a green suede jacket. The jacket has a classic design with a pointed collar, a button-up front, and two chest pockets with flap closures. The suede material gives it a textured appearance, and the overall style is casual yet stylish. The jacket also features side pockets and a straight hem.\"],[\"How can I style the image?\",\"To style the green suede jacket, consider the following options:\\n\\n1. **Casual Outfit**: Pair the jacket with a simple white t-shirt and blue jeans. Add a pair of white sneakers for a relaxed, everyday look.\\n\\n2. **Smart-Casual**: Wear the jacket over a button-down shirt and chinos. Choose a pair of brown leather shoes to complement the suede material.\\n\\n3. **Layering**: Layer the jacket over a hoodie or a sweater for a more textured look. This works well in cooler weather.\\n\\n4. **Accessories**: Add a leather belt that matches the jacket's color, a watch, and a pair of sunglasses to complete the look.\\n\\n5. **Footwear**: Depending on the occasion, you can choose between boots, sneakers, or dress shoes to match the jacket.\\n\\n6. **Seasonal Considerations**: The suede material is ideal for fall and winter, but you can also wear it in spring with lighter layers.\\n\\n7. **Color Coordination**: Since the jacket is green, consider pairing it with neutral colors like beige, brown, or black to make the green stand out.\\n\\n8. **Formal Touch**: For a more formal look, pair the jacket with a dress shirt and a tie, along with dress pants and leather shoes.\\n\\nThese styling tips should help you create a variety of looks with the green suede jacket.\"]],\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#   file=None,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#   api_name=\"/add_text\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     17\u001b[0m client\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m     18\u001b[0m   api_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/reset_user_input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m  \u001b[49m\u001b[43m_chatbot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m  \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/predict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/gradio_client/client.py:476\u001b[0m, in \u001b[0;36mClient.predict\u001b[0;34m(self, api_name, fn_index, *args, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;124;03mCalls the Gradio API and returns the result (this is a blocking call).\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m    >> 9.0\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_fn_index(api_name, fn_index)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m--> 476\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/gradio_client/client.py:1509\u001b[0m, in \u001b[0;36mJob.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;124;03m    Return the result of the call that the future represents. Raises CancelledError: If the future was cancelled, TimeoutError: If the future didn't finish executing before the given timeout, and Exception: If the call raised then that exception will be raised.\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;124;03m        >> 9\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/gradio_client/client.py:1128\u001b[0m, in \u001b[0;36mEndpoint.make_end_to_end_fn.<locals>._inner\u001b[0;34m(*data)\u001b[0m\n\u001b[1;32m   1126\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minsert_empty_state(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[1;32m   1127\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_input_files(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m-> 1128\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_predictions(\u001b[38;5;241m*\u001b[39mpredictions)\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Append final output only if not already present\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# for consistency between generators and not generators\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/gradio_client/client.py:1240\u001b[0m, in \u001b[0;36mEndpoint.make_predict.<locals>._predict\u001b[0;34m(*data)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1240\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AppError(\n\u001b[1;32m   1241\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe upstream Gradio app has raised an exception but has not enabled \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1242\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose error reporting. To enable, set show_error=True in launch().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1243\u001b[0m         )\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AppError(\n\u001b[1;32m   1246\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe upstream Gradio app has raised an exception: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1247\u001b[0m             \u001b[38;5;241m+\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1248\u001b[0m         )\n",
      "\u001b[0;31mAppError\u001b[0m: The upstream Gradio app has raised an exception but has not enabled verbose error reporting. To enable, set show_error=True in launch()."
     ]
    }
   ],
   "source": [
    "from gradio_client import Client, file\n",
    "\n",
    "client = Client(\"Qwen/Qwen2-VL\")\n",
    "\n",
    "client.predict(\n",
    "  api_name=\"/reset_state\"\n",
    ")\n",
    "\n",
    "client.predict(\n",
    "  history=[],\n",
    "  text=None,\n",
    "  api_name=\"/add_text\"\n",
    ")\n",
    "\n",
    "client.predict(\n",
    "  api_name=\"/reset_user_input\"\n",
    ")\n",
    "\n",
    "client.predict(\n",
    "  _chatbot=[[\"Hello\",None]],\n",
    "  api_name=\"/predict\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
