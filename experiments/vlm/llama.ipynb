{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=\"\"\n",
    "HUGGINGFACE_API_KEY=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_identifier_prompt = \"\"\"\n",
    "You are part of a team tasked with generating role-aware and context-aware image alt texts for images on websites. Your role is to identify the role of the given image in the website according to the definitions provided by the WCAG Web Accessibility Initiative (WAI) outlined below.\n",
    "\n",
    "\n",
    "1. informative: Images that graphically represent concepts and information, typically pictures, photos, and illustrations. The text alternative should be at least a short description conveying the essential information presented by the image.\n",
    "\n",
    "2. decorative: The only purpose of an image is to add visual decoration to the page, rather than to convey information that is important to understanding the page. This includes images that are considered eye candy or used for visual effect. Classify the image to decorative if having a null alt-text (alt=\"\") will not result in any loss of information.\n",
    "\n",
    "3. functional: Images used as a link or as a button, which carry a functionality to the page. Examples of such images are a printer icon to represent the print function or a button to submit a form. The alt text should describe the functionality of the link or button rather than the visual image.\n",
    "\n",
    "4. complex: Images used to convey data or detailed information, such as graphs or charts. Alt texts provide a complete text equivalent of the data or information provided in the image as the text alternative.\n",
    "\n",
    "\n",
    "As each role needs to be handled differently when generating alt texts, your output will be used to help another team member write the most suitable alt text that is role-aware and contex-aware for the image to help create more accessible websites.\n",
    "\n",
    "Return only the role of the image from the list above. Return the role as a single word without any enclosing bracket, e.g., informative, decorative, functional, text, or complex. THIS IS IMPORTANT! RETURN ONLY THE ROLE OF THE IMAGE.\n",
    "\n",
    "You are given the details of the image found on a website as follows:\n",
    "{message}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "json_dir = \"../../scraper/output\"\n",
    "\n",
    "filenames = os.listdir(json_dir)\n",
    "\n",
    "# Shuffle the filenames\n",
    "random.seed(42)\n",
    "random.shuffle(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(api_key=HUGGINGFACE_API_KEY)\n",
    "\n",
    "def determine_role_llama(image):\n",
    "    image_details = f\"\"\"\n",
    "        The image's attributes: {json.dumps(image[\"attrs\"])}\\n\\n\n",
    "        The image's <a> or <button> parent: {image[\"a_button_parent\"]}\\n\\n\n",
    "        The previous text before the image appears: {image[\"previous_text\"]}\\n\\n\n",
    "        The next text after the image appears: {image[\"next_text\"]}\\n\\n\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image[\"src\"]\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": role_identifier_prompt.format(message=image_details)\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-11B-Vision-Instruct\", \n",
    "        messages=messages, \n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def determine_role_gpt_4o(image):\n",
    "    image_details = f\"\"\"\n",
    "        The image's attributes: {json.dumps(image[\"attrs\"])}\\n\\n\n",
    "        The image's <a> or <button> parent: {image[\"a_button_parent\"]}\\n\\n\n",
    "        The previous text before the image appears: {image[\"previous_text\"]}\\n\\n\n",
    "        The next text after the image appears: {image[\"next_text\"]}\\n\\n\n",
    "    \"\"\"\n",
    "        \n",
    "    role_identifier_llm = ChatOpenAI(model='gpt-4o', temperature=0.5, api_key=OPENAI_API_KEY)\n",
    "    predicted_role = role_identifier_llm.invoke(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                role_identifier_prompt\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                [\n",
    "                    {\n",
    "                        \"type\": \"image_url\", \"image_url\": {\"url\": image[\"src\"]}\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\", \"text\": image_details\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return predicted_role.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def determine_role_gpt_4o(image):\n",
    "    image_details = f\"\"\"\n",
    "        The image's attributes: {json.dumps(image[\"attrs\"])}\\n\\n\n",
    "        The image's <a> or <button> parent: {image[\"a_button_parent\"]}\\n\\n\n",
    "        The previous text before the image appears: {image[\"previous_text\"]}\\n\\n\n",
    "        The next text after the image appears: {image[\"next_text\"]}\\n\\n\n",
    "    \"\"\"\n",
    "        \n",
    "    role_identifier_llm = ChatOpenAI(model='gpt-4o-mini', temperature=0.5, api_key=OPENAI_API_KEY)\n",
    "    predicted_role = role_identifier_llm.invoke(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                role_identifier_prompt\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                [\n",
    "                    {\n",
    "                        \"type\": \"image_url\", \"image_url\": {\"url\": image[\"src\"]}\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\", \"text\": image_details\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return predicted_role.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(results):\n",
    "    scores = {\n",
    "        \"llama\": {\n",
    "            \"whole_accuracy\": 0,\n",
    "            \"precision\": {},\n",
    "            \"recall\": {},\n",
    "            \"f1\": {},\n",
    "        },\n",
    "        \"gpt-4o\": {\n",
    "            \"whole_accuracy\": 0,\n",
    "            \"precision\": {},\n",
    "            \"recall\": {},\n",
    "            \"f1\": {},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for model in results[\"details\"]:\n",
    "        # Whole accuracy\n",
    "        scores[model][\"whole_accuracy\"] = sum([results[\"details\"][model][role][\"true_positive\"] for role in results[\"details\"][model]]) / results[\"total_images\"]\n",
    "\n",
    "        # Accuracy, Precision, Recall, F1 for each role\n",
    "        for role in results[\"details\"][model]:\n",
    "            true_positive = results[\"details\"][model][role][\"true_positive\"]\n",
    "            false_positive = results[\"details\"][model][role][\"false_positive\"]\n",
    "            false_negative = results[\"details\"][model][role][\"false_negative\"]\n",
    "\n",
    "            scores[model][\"precision\"][role] = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0\n",
    "            scores[model][\"recall\"][role] = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0\n",
    "            scores[model][\"f1\"][role] = 2 * (scores[model][\"precision\"][role] * scores[model][\"recall\"][role]) / (scores[model][\"precision\"][role] + scores[model][\"recall\"][role]) if scores[model][\"precision\"][role] + scores[model][\"recall\"][role] > 0 else 0\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hannarubbercompany.com.json...\n",
      "Correct role: functional\n",
      "Predicted role (Llama): informative\n",
      "Predicted role (GPT-4o): functional\n",
      "Total images processed: 1\n",
      "Results: {'total_images': 1, 'details': {'llama': {'informative': {'true_positive': 0, 'false_positive': 1, 'false_negative': 0}, 'decorative': {'true_positive': 0, 'false_positive': 0, 'false_negative': 0}, 'functional': {'true_positive': 0, 'false_positive': 0, 'false_negative': 1}, 'complex': {'true_positive': 0, 'false_positive': 0, 'false_negative': 0}}, 'gpt-4o': {'informative': {'true_positive': 0, 'false_positive': 0, 'false_negative': 0}, 'decorative': {'true_positive': 0, 'false_positive': 0, 'false_negative': 0}, 'functional': {'true_positive': 1, 'false_positive': 0, 'false_negative': 0}, 'complex': {'true_positive': 0, 'false_positive': 0, 'false_negative': 0}}}}\n",
      "Scores: {'llama': {'whole_accuracy': 0.0, 'precision': {'informative': 0.0, 'decorative': 0, 'functional': 0, 'complex': 0}, 'recall': {'informative': 0, 'decorative': 0, 'functional': 0.0, 'complex': 0}, 'f1': {'informative': 0, 'decorative': 0, 'functional': 0, 'complex': 0}}, 'gpt-4o': {'whole_accuracy': 1.0, 'precision': {'informative': 0, 'decorative': 0, 'functional': 1.0, 'complex': 0}, 'recall': {'informative': 0, 'decorative': 0, 'functional': 1.0, 'complex': 0}, 'f1': {'informative': 0, 'decorative': 0, 'functional': 1.0, 'complex': 0}}}\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"total_images\": 0,\n",
    "    \"details\": {\n",
    "        \"llama\" : {\n",
    "            \"informative\": {\"true_positive\": 0, \"false_positive\": 0, \"false_negative\": 0},\n",
    "            \"decorative\": {\"true_positive\": 0, \"false_positive\": 0, \"false_negative\": 0},\n",
    "            \"functional\": {\"true_positive\": 0, \"false_positive\": 0, \"false_negative\": 0},\n",
    "            \"complex\": {\"true_positive\": 0, \"false_positive\": 0, \"false_negative\": 0}\n",
    "        },\n",
    "        \"gpt-4o\" : {\n",
    "            \"informative\": {\"true_positive\": 0, \"false_positive\": 0, \"false_negative\": 0},\n",
    "            \"decorative\": {\"true_positive\": 0, \"false_positive\": 0, \"false_negative\": 0},\n",
    "            \"functional\": {\"true_positive\": 0, \"false_positive\": 0, \"false_negative\": 0},\n",
    "            \"complex\": {\"true_positive\": 0, \"false_positive\": 0, \"false_negative\": 0}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "for filename in os.listdir(json_dir)[0:1]:\n",
    "    if filename.endswith(\".json\"):\n",
    "        try:\n",
    "            # Read the JSON file\n",
    "            with open(os.path.join(json_dir, filename), \"r\") as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            print(f\"Processing {filename}...\")\n",
    "            \n",
    "            # Extract the image link and textual context from the JSON data\n",
    "            whole_text = data[\"text\"]\n",
    "            sub_images = data[\"images\"]\n",
    "\n",
    "            for image in sub_images[0:1]:\n",
    "                if image[\"role\"] != \"informative\" and image[\"role\"] != \"decorative\" and image[\"role\"] != \"functional\" and image[\"role\"] != \"complex\":\n",
    "                    continue\n",
    "\n",
    "                llama_answer = determine_role_llama(image).strip().lower().replace(\".\", \"\")\n",
    "                gpt_4o_answer = determine_role_gpt_4o(image).strip().lower().replace(\".\", \"\")\n",
    "                print(\"Correct role:\", image[\"role\"])\n",
    "                print(\"Predicted role (Llama):\", llama_answer)\n",
    "                print(\"Predicted role (GPT-4o):\", gpt_4o_answer)\n",
    "\n",
    "                results[\"total_images\"] += 1\n",
    "                if llama_answer == image[\"role\"]:\n",
    "                    results[\"details\"][\"llama\"][image[\"role\"]][\"true_positive\"] += 1\n",
    "                else:\n",
    "                    results[\"details\"][\"llama\"][image[\"role\"]][\"false_negative\"] += 1\n",
    "                    if llama_answer in results[\"details\"][\"llama\"]:\n",
    "                        results[\"details\"][\"llama\"][llama_answer][\"false_positive\"] += 1\n",
    "\n",
    "                if gpt_4o_answer == image[\"role\"]:\n",
    "                    results[\"details\"][\"gpt-4o\"][image[\"role\"]][\"true_positive\"] += 1\n",
    "                else:\n",
    "                    results[\"details\"][\"gpt-4o\"][image[\"role\"]][\"false_negative\"] += 1\n",
    "                    if gpt_4o_answer in results[\"details\"][\"gpt-4o\"]:\n",
    "                        results[\"details\"][\"gpt-4o\"][gpt_4o_answer][\"false_positive\"] += 1\n",
    "\n",
    "            # Save the final state to a JSON file\n",
    "            with open(f\"./output/llama-complete.json\", \"w\") as file:\n",
    "                json.dump({\n",
    "                    \"results\": results,\n",
    "                    \"scores\": calculate_scores(results)\n",
    "                }, file, indent=4)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "\n",
    "print(\"Total images processed:\", results[\"total_images\"])\n",
    "print(\"Results:\", results)\n",
    "print(\"Scores:\", calculate_scores(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
